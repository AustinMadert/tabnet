import torch
import torch.nn as nn

from feature_transformer import FeatureTransformer


class AttentiveTransformer(nn.Module):

    def __init__(self) -> None:
        super().__init__()


    def forward(self, X: torch.Tensor) -> torch.Tensor:
        return X


class TabNetEncoder(nn.Module):

    def __init__(self) -> None:
        super().__init__()


    def forward(self, X: torch.Tensor) -> torch.Tensor:
        return X